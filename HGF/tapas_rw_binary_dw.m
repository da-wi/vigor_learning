function [traj, infStates] = tapas_rw_binary_dw(r, p, varargin)
% Calculates the trajectories of v under the Rescorla-Wagner learning model
%
% This function can be called in two ways:
% 
% (1) tapas_rw_binary(r, p)
%   
%     where r is the structure generated by tapas_fitModel and p is the parameter vector in native space;
%
% (2) tapas_rw_binary(r, ptrans, 'trans')
% 
%     where r is the structure generated by tapas_fitModel, ptrans is the parameter vector in
%     transformed space, and 'trans' is a flag indicating this.
%
% --------------------------------------------------------------------------------------------------
% Adapted for MID by David Willinger, UZH, 2020
%
% This file is part of the HGF toolbox, which is released under the terms of the GNU General Public
% Licence (GPL), version 3. You can redistribute it and/or modify it under the terms of the GPL
% (either version 3 or, at your option, any later version). For further details, see the file
% COPYING or <http://www.gnu.org/licenses/>.

% Transform paramaters back to their native space if needed
if ~isempty(varargin) && strcmp(varargin{1},'trans');
    p = tapas_rw_binary_transp(r, p);
end

% Unpack parameters
v_0 = p(1);
al  = p(2);

% Add dummy "zeroth" trial
cue_value  = [0; r.u(:,1)];
outcome    = [0; r.u(:,2)];
repetition = [0; diff(r.u(:,1)) == 0];

%u = [0; r.u(:,1)];
n = length(outcome);

% Initialize updated quantity: value
pGain  = NaN(n,1);
pe = NaN(n,1);

%  Kumar et al., 2018 Neuropsychopharmacology: 
% A positive RPE beta identifies a brain region with higher activation for
% unexpected reward and lower activation for unexpected omission of rewards during gain condition
% (trials); conversely, a positive PPE beta identifies a brain region with higher activation for
% unexpected punishment and lower activation for unexpected omission of punishment during loss
% condition (trials)
rpe = NaN(n,1);  % Yacubian et al., 2006 J Neurosc, Kumar et al., 2018 Neuropsychopharmacology
ppe = NaN(n,1);  % Yacubian et al., 2006 J Neurosc, Kumar et al., 2018 Neuropsychopharmacology

ev = NaN(n,1);
ev_adj = NaN(n,1); % Rigoli et al., 2016, NeuroImage
ev_pos = NaN(n,1); % Yacubian et al., 2006 J Neurosc = 0 @loss trials
ev_neg = NaN(n,1); % Yacubian et al., 2006 J Neurosc = 0 @reward trials

avg_reward = NaN(n,1);
avg_punish = NaN(n,1);
avg_outcome = NaN(n,1);
% surprise = NaN(n,1);

% Prior
pGain(1) = v_0;
avg_reward(1) = 0;
avg_punish(1) = 0;
avg_outcome(1) = 0;

% Pass through value update loop
for k = 2:1:n
    if not(ismember(k-1, r.ign))
        
        %%%%%%%%%%%%%%%%%%%%%%
        % Effect of input u(k)
        %%%%%%%%%%%%%%%%%%%%%%
        
        % Expected value
        if cue_value(k) > 0
            % gain trials
            ev(k) = cue_value(k)*pGain(k-1);
            % see Rigoli 2016
            ev_adj(k) = cue_value(k)*pGain(k-1);
            ev_pos(k) = cue_value(k)*pGain(k-1);
            ev_neg(k) = 0;
        else
            % lose trials
            ev(k) = cue_value(k)*(1-pGain(k-1));
            % see Rigoli 2016
            ev_adj(k) =  cue_value(k)*(1-pGain(k-1)) - cue_value(k)*pGain(k-1);
            ev_pos(k) = 0;
            ev_neg(k) = cue_value(k)*(1-pGain(k-1));
        end
            
        % Prediction error
        pe(k)  = outcome(k)-ev(k);
        
        if cue_value(k) > 0
            rpe(k) = pe(k);
            ppe(k) = 0;
        elseif cue_value(k) < 0
            rpe(k) = 0;
            ppe(k) = -pe(k);
        end    
        
        %avg_reward(k) = avg_reward(k-1) + (outcome(k-1)-avg_reward(k-1));
        %avg_punish(k) = avg_punish(k-1) + (avg_punish(k-1)-outcome(k-1));
        
        % Average reward and punishment
        % Calculate average as in https://www.nature.com/articles/npp201348
        if cue_value(k-1) > 0
            % gain trials
            avg_reward(k) = avg_reward(k-1) + al*(outcome(k-1)-avg_reward(k-1));
            avg_punish(k) = avg_punish(k-1);
        elseif cue_value(k-1) < 0
            % lose trials
            avg_reward(k) = avg_reward(k-1);
            avg_punish(k) = avg_punish(k-1) + al*(abs(outcome(k-1))-avg_punish(k-1));
        elseif cue_value(k-1) == 0
            % neutral trials
            avg_reward(k) = avg_reward(k-1);
            avg_punish(k) = avg_punish(k-1);
        end
        avg_outcome(k) = avg_outcome(k-1) + al*(outcome(k-1)-avg_outcome(k-1));
            
        % pGain
        if cue_value(k)==0
            pGain(k) = pGain(k-1);
        else
            pGain(k) = pGain(k-1) + al * (pe(k)/abs(cue_value(k)));
        end
        
    else
        pe(k) = 0;
        pGain(k)  = pGain(k-1);
    end
end

% Predicted value
vhat = pGain;
vhat(end) = [];

% Calculate correct
correct = cue_value + outcome;
correct(correct == 0) = nan;
correct(correct == 1 | correct == 4 | correct == -2 | correct == -8) = 0;
correct(correct == -1 | correct == -4 | correct == 2 | correct == 8) = 1;

% Remove representation priors
pGain(1)  = [];
pe(1) = [];
rpe(1) = [];
ppe(1) = [];
ev(1) = [];
ev_adj(1) = [];
ev_pos(1) = [];
ev_neg(1) = [];
correct(1) = [];
cue_value(1) = [];
outcome(1) = [];
avg_reward(1) = [];
avg_punish(1) = [];
avg_outcome(1) = [];
% surprise(1) = [];
       
% Available reward and punishment
% as in https://www.nature.com/articles/npp201348
available_reward = cue_value;
available_reward( find(cue_value < 0) ) = 0;
available_punishment = cue_value;
available_punishment( find(cue_value > 0) ) = 0;
available_punishment = abs(available_punishment);

% Create result data structure
traj = struct;

traj.v     = pGain;
traj.vhat  = vhat;
traj.da    = pe;
traj.rpe    = rpe;
traj.ppe    = ppe;
traj.ev    = ev;
traj.ev_adj    = ev_adj;
traj.ev_pos    = ev_pos;
traj.ev_neg    = ev_neg;
traj.correct = correct;
traj.cue_value = cue_value;
traj.outcome   = outcome;
traj.available_reward = available_reward;
traj.available_punishment = available_punishment;
traj.avg_reward = avg_reward;
traj.avg_punish = avg_punish;
traj.avg_outcome = avg_outcome;
% traj.surprise = surprise;
traj.repetition = repetition;

% Create matrix (in this case: vector) needed by observation model
infStates(:,1) = traj.vhat;
infStates(:,2) = traj.da;
infStates(:,3) = traj.ev;
infStates(:,4) = traj.ev_adj;
infStates(:,5) = traj.correct;
infStates(:,6) = traj.cue_value;
infStates(:,7) = traj.outcome;
infStates(:,8) = traj.avg_outcome;
infStates(:,9) = traj.avg_reward;
infStates(:,10) = traj.avg_punish;
infStates(:,11) = traj.available_reward;
infStates(:,12) = traj.available_punishment;
infStates(:,13) = traj.repetition;
infStates(:,14) = traj.rpe;
infStates(:,15) = traj.ppe;
infStates(:,16) = traj.ev_pos;
infStates(:,17) = traj.ev_neg;

return;
